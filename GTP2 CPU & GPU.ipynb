{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\n\n# For reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Define cache directory (adjust this path if needed for Colab)\ncache_dir = \"/content/cache\"\n\n# Use GPU if available (T4 GPU on Colab)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:01:19.123711Z","iopub.execute_input":"2025-04-08T08:01:19.124169Z","iopub.status.idle":"2025-04-08T08:01:24.350686Z","shell.execute_reply.started":"2025-04-08T08:01:19.124124Z","shell.execute_reply":"2025-04-08T08:01:24.349286Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install pyRAPL\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport cProfile\nimport pyRAPL\nimport subprocess\n\n# Configuration\nMODEL_NAME = \"gpt2\"\nBATCH_SIZE = 8\nMAX_LENGTH = 512\nPRUNE_RATIO = 0.5\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\nmodel.eval()\n\n# Load dataset\ndataset = load_dataset(\"wikitext\", \"wikitext-2-v1\", split=\"test\")\n\n# Tokenize\ndef tokenize(example):\n    return tokenizer(example['text'], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n\ntokenized = dataset.map(tokenize, batched=True)\ninput_ids = torch.tensor(tokenized[\"input_ids\"][:BATCH_SIZE]).to(DEVICE)\nattention_mask = torch.tensor(tokenized[\"attention_mask\"][:BATCH_SIZE]).to(DEVICE)\n\n# Forward hook to capture activations\nactivation_store = {}\n\ndef get_activation(name):\n    def hook(module, input, output):\n        activation_store[name] = input[0].detach()\n    return hook\n\nfor name, module in model.named_modules():\n    if isinstance(module, nn.Linear):\n        module.register_forward_hook(get_activation(name))\n\n# Run one forward pass to capture activations\nwith torch.no_grad():\n    _ = model(input_ids=input_ids, attention_mask=attention_mask)\n\n# Wanda pruning logic\ndef wanda_prune(model, prune_ratio):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear) and name in activation_store:\n            W = module.weight.data\n            A = activation_store[name]  # shape: (batch, in_features)\n            A = A.view(-1, A.shape[-1])  # Flatten batch and seq dims â†’ (B*T, hidden_dim)\n            avg_A = torch.abs(A).mean(dim=0)  # Shape: (in_features,)\n            scores = torch.abs(W) * avg_A.unsqueeze(0)  # broadcasting\n\n            # Flatten and find the global threshold\n            num_params = scores.numel()\n            k = int((1 - prune_ratio) * num_params)\n            topk_scores, _ = torch.topk(scores.view(-1), k, largest=True, sorted=False)\n            threshold = topk_scores.min()\n            mask = scores >= threshold\n\n\n            module.weight.data *= mask  # Zero out pruned weights\nstart_time = time.time()\nwanda_prune(model, PRUNE_RATIO)\n\n# Evaluate perplexity\ndef evaluate_perplexity(model, input_ids, attention_mask):\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss\n    return torch.exp(loss).item()\n\nperplexity = evaluate_perplexity(model, input_ids, attention_mask)\nprint(f\"Perplexity after pruning: {perplexity:.2f}\")\n\n# Profile CPU usage\ndef profile_cpu():\n    profiler = cProfile.Profile()\n    profiler.enable()\n    evaluate_perplexity(model, input_ids, attention_mask)\n    profiler.disable()\n    profiler.print_stats(sort=\"cumtime\")\n\nprofile_cpu()\n\n\n\ndef estimate_cpu_energy(runtime_seconds, tdp_watts=65):\n    \"\"\"TDP is your CPU's max power (e.g., 65W for Intel i5).\"\"\"\n    return tdp_watts * runtime_seconds / 3600  # Energy in Wh\n\nruntime = time.time() - start_time\nprint(f\"Estimated CPU Energy: {estimate_cpu_energy(runtime):.2f} Wh\")\n\ndef get_gpu_energy():\n    result = subprocess.run(\n        [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\"],\n        capture_output=True, text=True\n    )\n    return float(result.stdout.strip())\n\nprint(f\"GPU Power Draw: {get_gpu_energy()} W\")\n\n# Profile GPU (time & memory)\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        evaluate_perplexity(model, input_ids, attention_mask)\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# Track energy (CPU) with pyRAPL\npyRAPL.setup()\n@pyRAPL.measureit\ndef energy_profile():\n    return evaluate_perplexity(model, input_ids, attention_mask)\nenergy_profile()\n\n# Track GPU energy (Watt draw)\ndef gpu_energy():\n    result = subprocess.check_output([\n        \"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\"\n    ])\n    print(f\"GPU Power Draw (Watts): {result.decode().strip()}\")\n\ngpu_energy()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-08T08:02:05.367567Z","iopub.execute_input":"2025-04-08T08:02:05.367905Z"}},"outputs":[{"name":"stdout","text":"Collecting pyRAPL\n  Downloading pyRAPL-0.2.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\nDownloading pyRAPL-0.2.3.1-py2.py3-none-any.whl (27 kB)\nInstalling collected packages: pyRAPL\nSuccessfully installed pyRAPL-0.2.3.1\n","output_type":"stream"}],"execution_count":null}]}